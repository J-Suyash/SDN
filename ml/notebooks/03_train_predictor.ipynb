{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDN ML Traffic Management - Congestion Predictor Training\n",
    "\n",
    "This notebook trains a model to predict network congestion based on:\n",
    "- Historical utilization patterns\n",
    "- Time-of-day features\n",
    "- Trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload link_timeseries.csv if not present\n",
    "try:\n",
    "    df = pd.read_csv('link_timeseries.csv')\n",
    "    print(f\"Loaded link_timeseries.csv: {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Upload link_timeseries.csv:\")\n",
    "    uploaded = files.upload()\n",
    "    df = pd.read_csv('link_timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data if needed\n",
    "if len(df) < 100:\n",
    "    print(\"Creating synthetic training data...\")\n",
    "    \n",
    "    # Simulate 7 days of data at 10-second intervals\n",
    "    n_samples = 7 * 24 * 360  # 7 days\n",
    "    timestamps = pd.date_range('2026-01-01', periods=n_samples, freq='10s')\n",
    "    \n",
    "    # Base utilization with daily pattern\n",
    "    hours = timestamps.hour + timestamps.minute / 60\n",
    "    \n",
    "    # Create realistic daily pattern:\n",
    "    # - Low at night (0-6)\n",
    "    # - Peak at 9AM (login surge)\n",
    "    # - High during work hours (9-17)\n",
    "    # - Decrease after work\n",
    "    base_util = np.zeros(n_samples)\n",
    "    for i, h in enumerate(hours):\n",
    "        if 0 <= h < 6:\n",
    "            base_util[i] = 0.1 + np.random.uniform(0, 0.1)\n",
    "        elif 6 <= h < 8:\n",
    "            base_util[i] = 0.2 + (h - 6) * 0.15 + np.random.uniform(0, 0.1)\n",
    "        elif 8 <= h < 10:  # Morning peak\n",
    "            base_util[i] = 0.6 + np.random.uniform(0, 0.3)\n",
    "        elif 10 <= h < 12:\n",
    "            base_util[i] = 0.5 + np.random.uniform(0, 0.2)\n",
    "        elif 12 <= h < 14:  # Lunch dip\n",
    "            base_util[i] = 0.4 + np.random.uniform(0, 0.15)\n",
    "        elif 14 <= h < 17:\n",
    "            base_util[i] = 0.5 + np.random.uniform(0, 0.2)\n",
    "        elif 17 <= h < 19:\n",
    "            base_util[i] = 0.4 + np.random.uniform(0, 0.15)\n",
    "        else:\n",
    "            base_util[i] = 0.2 + np.random.uniform(0, 0.1)\n",
    "    \n",
    "    # Add weekend effect (lower traffic)\n",
    "    is_weekend = timestamps.dayofweek >= 5\n",
    "    base_util[is_weekend] *= 0.5\n",
    "    \n",
    "    # Add some random spikes\n",
    "    spike_indices = np.random.choice(n_samples, size=n_samples // 100, replace=False)\n",
    "    base_util[spike_indices] += np.random.uniform(0.2, 0.4, len(spike_indices))\n",
    "    \n",
    "    # Clip to valid range\n",
    "    base_util = np.clip(base_util, 0, 1)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'switch': 's1',\n",
    "        'port': 1,\n",
    "        'utilization': base_util,\n",
    "        'bytes_delta': (base_util * 10_000_000 * 10 / 8).astype(int),  # 10Mbps link, 10s interval\n",
    "        'hour_of_day': timestamps.hour,\n",
    "        'minute_of_hour': timestamps.minute,\n",
    "        'is_weekday': ~is_weekend,\n",
    "    })\n",
    "    \n",
    "    # Create label: congested in next interval\n",
    "    df['label'] = (df['utilization'].shift(-1) > 0.7).astype(int).fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Created {len(df)} synthetic samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse timestamp if needed\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "    df['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekday'] = df['day_of_week'] < 5\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (previous utilization values)\n",
    "LAG_STEPS = 6  # Use last 6 intervals (1 minute of history)\n",
    "\n",
    "for i in range(1, LAG_STEPS + 1):\n",
    "    df[f'util_lag_{i}'] = df['utilization'].shift(i)\n",
    "\n",
    "# Rolling statistics\n",
    "df['util_rolling_mean'] = df['utilization'].rolling(window=6).mean()\n",
    "df['util_rolling_std'] = df['utilization'].rolling(window=6).std()\n",
    "df['util_rolling_max'] = df['utilization'].rolling(window=6).max()\n",
    "\n",
    "# Trend feature\n",
    "df['util_trend'] = df['utilization'] - df['util_lag_1']\n",
    "\n",
    "# Drop rows with NaN (from lag features)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"After feature engineering: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "feature_columns = [\n",
    "    'util_lag_1', 'util_lag_2', 'util_lag_3', 'util_lag_4', 'util_lag_5', 'util_lag_6',\n",
    "    'util_rolling_mean', 'util_rolling_std', 'util_rolling_max',\n",
    "    'util_trend',\n",
    "    'hour_of_day', 'minute_of_hour',\n",
    "]\n",
    "\n",
    "if 'is_weekday' in df.columns:\n",
    "    feature_columns.append('is_weekday')\n",
    "\n",
    "print(\"Features:\", feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[feature_columns].copy()\n",
    "\n",
    "# Convert boolean to int\n",
    "if 'is_weekday' in X.columns:\n",
    "    X['is_weekday'] = X['is_weekday'].astype(int)\n",
    "\n",
    "# Target: next interval utilization (regression) or congestion (classification)\n",
    "y_regression = df['utilization'].shift(-1).fillna(df['utilization'].iloc[-1])\n",
    "y_classification = (y_regression > 0.7).astype(int)  # Congested if > 70%\n",
    "\n",
    "print(f\"Congestion rate: {y_classification.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (time-series aware - don't shuffle)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train_reg = y_regression.iloc[:split_idx]\n",
    "y_test_reg = y_regression.iloc[split_idx:]\n",
    "y_train_cls = y_classification.iloc[:split_idx]\n",
    "y_test_cls = y_classification.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Regression Model (Predict Utilization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regressor\n",
    "regressor = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training regressor...\")\n",
    "regressor.fit(X_train, y_train_reg)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regressor\n",
    "y_pred_reg = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Regression Metrics:\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  MAE: {mae:.6f}\")\n",
    "print(f\"  R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test_reg, y_pred_reg, alpha=0.3, s=10)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "axes[0].axhline(y=0.7, color='orange', linestyle=':', label='Congestion threshold')\n",
    "axes[0].axvline(x=0.7, color='orange', linestyle=':')\n",
    "axes[0].set_xlabel('Actual Utilization')\n",
    "axes[0].set_ylabel('Predicted Utilization')\n",
    "axes[0].set_title('Predicted vs Actual')\n",
    "axes[0].legend()\n",
    "\n",
    "# Time series (last 500 samples)\n",
    "n_plot = min(500, len(y_test_reg))\n",
    "axes[1].plot(y_test_reg.values[-n_plot:], label='Actual', alpha=0.7)\n",
    "axes[1].plot(y_pred_reg[-n_plot:], label='Predicted', alpha=0.7)\n",
    "axes[1].axhline(y=0.7, color='red', linestyle='--', label='Congestion threshold')\n",
    "axes[1].set_xlabel('Time Index')\n",
    "axes[1].set_ylabel('Utilization')\n",
    "axes[1].set_title('Prediction Over Time')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Classification Model (Predict Congestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    class_weight='balanced',  # Handle imbalanced classes\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "classifier.fit(X_train, y_train_cls)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier\n",
    "y_pred_cls = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cls, y_pred_cls, target_names=['Normal', 'Congested']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_cls, y_pred_cls)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Congested'],\n",
    "            yticklabels=['Normal', 'Congested'])\n",
    "plt.title('Congestion Prediction Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': regressor.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Regressor):\")\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Congestion Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictor artifacts\n",
    "predictor_artifacts = {\n",
    "    'regressor': regressor,\n",
    "    'classifier': classifier,\n",
    "    'feature_columns': feature_columns,\n",
    "    'lag_steps': LAG_STEPS,\n",
    "    'congestion_threshold': 0.7,\n",
    "    'metrics': {\n",
    "        'regression_r2': r2,\n",
    "        'regression_mae': mae,\n",
    "        'classification_accuracy': accuracy,\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(predictor_artifacts, 'predictor.pkl')\n",
    "print(\"Models saved to predictor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "files.download('predictor.pkl')\n",
    "print(\"\\nDownload predictor.pkl and place it in ml/models/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_congestion(utilization_history, hour, minute, is_weekday=True):\n",
    "    \"\"\"\n",
    "    Predict if congestion will occur in the next interval.\n",
    "    \n",
    "    Args:\n",
    "        utilization_history: List of last 6 utilization values\n",
    "        hour: Current hour (0-23)\n",
    "        minute: Current minute (0-59)\n",
    "        is_weekday: Whether it's a weekday\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_utilization, will_be_congested, probability)\n",
    "    \"\"\"\n",
    "    # Pad history if needed\n",
    "    history = list(utilization_history)[-6:]\n",
    "    while len(history) < 6:\n",
    "        history.insert(0, history[0] if history else 0.5)\n",
    "    \n",
    "    # Calculate features\n",
    "    features = {\n",
    "        'util_lag_1': history[-1],\n",
    "        'util_lag_2': history[-2],\n",
    "        'util_lag_3': history[-3],\n",
    "        'util_lag_4': history[-4],\n",
    "        'util_lag_5': history[-5],\n",
    "        'util_lag_6': history[-6],\n",
    "        'util_rolling_mean': np.mean(history),\n",
    "        'util_rolling_std': np.std(history),\n",
    "        'util_rolling_max': np.max(history),\n",
    "        'util_trend': history[-1] - history[-2] if len(history) > 1 else 0,\n",
    "        'hour_of_day': hour,\n",
    "        'minute_of_hour': minute,\n",
    "        'is_weekday': int(is_weekday),\n",
    "    }\n",
    "    \n",
    "    X = pd.DataFrame([features])[feature_columns]\n",
    "    \n",
    "    # Predict\n",
    "    predicted_util = regressor.predict(X)[0]\n",
    "    congested_proba = classifier.predict_proba(X)[0][1]\n",
    "    will_be_congested = predicted_util > 0.7 or congested_proba > 0.5\n",
    "    \n",
    "    return predicted_util, will_be_congested, congested_proba\n",
    "\n",
    "# Test predictions\n",
    "test_cases = [\n",
    "    ([0.3, 0.35, 0.4, 0.45, 0.5, 0.55], 9, 0),   # Morning rise\n",
    "    ([0.6, 0.65, 0.7, 0.72, 0.75, 0.78], 10, 0), # Already congested\n",
    "    ([0.2, 0.2, 0.2, 0.2, 0.2, 0.2], 3, 0),      # Night, stable\n",
    "    ([0.4, 0.5, 0.6, 0.65, 0.68, 0.72], 14, 30), # Afternoon spike\n",
    "]\n",
    "\n",
    "print(\"Congestion Predictions:\")\n",
    "print(\"-\" * 70)\n",
    "for history, hour, minute in test_cases:\n",
    "    pred_util, congested, proba = predict_congestion(history, hour, minute)\n",
    "    print(f\"History: {history[-3:]}... @ {hour:02d}:{minute:02d}\")\n",
    "    print(f\"  Predicted utilization: {pred_util:.1%}\")\n",
    "    print(f\"  Congestion: {'YES' if congested else 'NO'} (prob: {proba:.1%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Download `predictor.pkl` and place it in `ml/models/` directory\n",
    "2. Run model evaluation notebook: `04_model_evaluation.ipynb`\n",
    "3. The orchestrator will automatically use trained models if present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
