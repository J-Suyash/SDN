{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDN ML Traffic Management - Data Exploration\n",
    "\n",
    "This notebook explores the collected datasets for training ML models.\n",
    "\n",
    "## Setup\n",
    "1. Upload `flows.csv` and `link_timeseries.csv` from your local `data/processed/` folder\n",
    "2. Run all cells to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab has most of these)\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import files\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files from local machine\n",
    "print(\"Upload flows.csv and link_timeseries.csv:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    flows_df = pd.read_csv('flows.csv')\n",
    "    print(f\"Loaded flows.csv: {len(flows_df)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"flows.csv not found - using sample data\")\n",
    "    # Create sample data for testing\n",
    "    flows_df = pd.DataFrame({\n",
    "        'flow_id': range(100),\n",
    "        'packet_count': np.random.randint(10, 10000, 100),\n",
    "        'byte_count': np.random.randint(1000, 1000000, 100),\n",
    "        'duration_sec': np.random.uniform(1, 120, 100),\n",
    "        'dst_port': np.random.choice([80, 443, 5001, 5002, 5003, 5000], 100),\n",
    "        'label': np.random.choice(['P0', 'P1', 'P2', 'P3'], 100, p=[0.2, 0.4, 0.2, 0.2])\n",
    "    })\n",
    "    flows_df['bytes_per_packet'] = flows_df['byte_count'] / flows_df['packet_count']\n",
    "    flows_df['packets_per_sec'] = flows_df['packet_count'] / flows_df['duration_sec']\n",
    "    flows_df['bytes_per_sec'] = flows_df['byte_count'] / flows_df['duration_sec']\n",
    "\n",
    "try:\n",
    "    links_df = pd.read_csv('link_timeseries.csv')\n",
    "    print(f\"Loaded link_timeseries.csv: {len(links_df)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"link_timeseries.csv not found - using sample data\")\n",
    "    # Create sample data\n",
    "    links_df = pd.DataFrame({\n",
    "        'timestamp': pd.date_range('2026-01-01', periods=500, freq='10s'),\n",
    "        'switch': 's1',\n",
    "        'port': 1,\n",
    "        'bytes_delta': np.random.randint(0, 1250000, 500),\n",
    "        'hour_of_day': np.random.randint(0, 24, 500),\n",
    "        'label': np.random.choice([0, 1], 500, p=[0.8, 0.2])\n",
    "    })\n",
    "    links_df['utilization'] = links_df['bytes_delta'] * 8 / (10_000_000 * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Flows Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Flows Dataset Info:\")\n",
    "print(flows_df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "flows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "print(\"Label Distribution:\")\n",
    "label_counts = flows_df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "label_counts.plot(kind='bar', ax=ax, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\n",
    "ax.set_title('Traffic Priority Class Distribution')\n",
    "ax.set_xlabel('Priority Class')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by class\n",
    "numeric_features = ['packet_count', 'byte_count', 'bytes_per_packet', \n",
    "                    'packets_per_sec', 'bytes_per_sec', 'duration_sec']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    if feature in flows_df.columns:\n",
    "        for label in flows_df['label'].unique():\n",
    "            data = flows_df[flows_df['label'] == label][feature]\n",
    "            axes[i].hist(data, bins=30, alpha=0.5, label=label)\n",
    "        axes[i].set_title(f'{feature} by Class')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = flows_df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = flows_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Link Timeseries Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Links Dataset Info:\")\n",
    "print(links_df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "links_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilization distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(links_df['utilization'], bins=50, edgecolor='black')\n",
    "axes[0].axvline(x=0.7, color='red', linestyle='--', label='Congestion threshold (70%)')\n",
    "axes[0].set_title('Link Utilization Distribution')\n",
    "axes[0].set_xlabel('Utilization')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "\n",
    "# Over time\n",
    "if 'timestamp' in links_df.columns:\n",
    "    links_df['timestamp'] = pd.to_datetime(links_df['timestamp'])\n",
    "    axes[1].plot(links_df['timestamp'], links_df['utilization'])\n",
    "    axes[1].axhline(y=0.7, color='red', linestyle='--', label='Congestion threshold')\n",
    "    axes[1].set_title('Utilization Over Time')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Utilization')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congestion by hour of day\n",
    "if 'hour_of_day' in links_df.columns:\n",
    "    hourly_util = links_df.groupby('hour_of_day')['utilization'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    hourly_util.plot(kind='bar')\n",
    "    plt.axhline(y=0.7, color='red', linestyle='--', label='Congestion threshold')\n",
    "    plt.title('Average Utilization by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Average Utilization')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congestion label distribution\n",
    "print(\"Congestion Label Distribution:\")\n",
    "print(links_df['label'].value_counts())\n",
    "print(f\"\\nCongestion rate: {links_df['label'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"FLOWS DATASET SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {len(flows_df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label in sorted(flows_df['label'].unique()):\n",
    "    count = len(flows_df[flows_df['label'] == label])\n",
    "    pct = count / len(flows_df) * 100\n",
    "    print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LINKS DATASET SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {len(links_df)}\")\n",
    "print(f\"Congestion rate: {links_df['label'].mean():.1%}\")\n",
    "print(f\"Average utilization: {links_df['utilization'].mean():.1%}\")\n",
    "print(f\"Max utilization: {links_df['utilization'].max():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train Classifier**: Open `02_train_classifier.ipynb` to train the traffic classifier\n",
    "2. **Train Predictor**: Open `03_train_predictor.ipynb` to train the congestion predictor\n",
    "3. **Evaluate Models**: Open `04_model_evaluation.ipynb` to evaluate model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
